In this lecture we will cover more of the structure of the internet, wewill go over some of the ways of calculating performance metrics.So, let's start first with access networks. In the context of networking,access networks serve as the crucial link between end-users and thebroader internet infrastructure. and Access networks are typically ownedand operated by Internet Service Providers (ISPs), such as cablecompanies and DSL providers. They provide the "last mile" service,connecting individual end-users to the broader telecommunications andinternet infrastructure. Access networks utilize various transmissionmedia to establish connections from end-users to the edge of theinternet. This includes technologies such as telephone lines, cable,copper, coaxial, fiber optics, and wireless communication. Modems play acrucial role in connecting end-user systems (e.g., computers,smartphones) to the access network. They facilitate the conversion ofdigital data from end-user devices into a format suitable fortransmission over the network. Edge routers, located at the edge of theaccess network, act as gateways to the internet core. They manage theflow of data between the access network and the broader internetinfrastructure.Here's another look at the structure or hierarchy of the internet: theaccess ISP may connect- up through several layers until you get to thetier 1 ISPs, tier 1 ISPs include big commercial providers such as AT&T,Sprint, and even content providers such as Google and so on. But youshould also notice that this hierarchy is not rigid and that sometimes anaccess ISP may connect directly to a tier 1 provider or directly to aninternet exchange point, it just depends how things have been set up.Let's talk a bit about network performance metrics; Network performancemetrics are crucial for assessing the efficiency, reliability, andoverall health of a computer network. There are three key metrics thatare often fundamental in evaluating network performance: the first isthroughput which we measure in bits per second. This is the rate at whichbits are being transferred between the sender and receiver.   We alsolook at an end-to-end delay so from sender to receiver and we refer tothis also as nodal delay sometimes. This is the summation of a number ofdifferent kinds of delay and it's complete end-to-end.  Finally packetloss which occurs when data packets transmitted across a network fail toreach their intended destination. So here let's talk about the four sources of packet delay that we addtogether to get the nodal delay. The first kind would be the nodalprocessing or just processing, that happens when a router receives apacket: it must first check for errors and determine if the packet isvalid, then it needs to determine the next hop or router to send thepacket to, so that takes some processing time.next we have queuing delay: if the network is congested at all there willbe a queue of packets waiting to be transmitted and that is queuingdelay. Then when the packet is ready to be sent, we have transmissiondelay: so, we have first the rate, which is or speed in bits per second(BPS) and we have L which is the packet length in bits, so ourtransmission delay is L over R and that works out to seconds. Finally,Propagation delay on the other hand is simply the time that the data, thebits, need to traverse or propagate through the media. This is close tothe speed of at or close to the speed of light depending on the media andD is the physical link in meters and s is the propagation speed of lightand so we get the delay of d over s. Once again, Transmission delayfocuses on the process of moving data through the output link of anetwork device, measuring the time it takes to complete thistransmission. On the other hand, propagation delay involves the physicaltravel time of a bit from the starting point of a link to the next hop inthe network. It's crucial to differentiate between these two types ofdelays, as they represent distinct aspects of data transfer within anetwork. Don’t mix them up. Let's talk about nodal delay and end to end delay. So here we have nodaldelay which is the sum of the four different kinds of delay we've talkedabout, processing delay, queuing delay, transmission delay, andpropagation delay between two nodes. Whereas end-to-end delay encompassesthe total delay from the source to the destination, including allintermediate nodes and additional delays.Let’s work on this example. We have three different times that a packetmust be transmitted to reach its destination, so it takes L over Rseconds to transmit from one computer to another, from one link toanother. The end to delay in this case then, if we as if we are ignoringall the other three delays, is three times the link transmit rate orthree times the transmitted delay. And if we're careful with our megabitsper second to bits per second conversion we wind up at 24 millisecondstransmitted. Note that in delay calculations we usually want to convertall the inputs to bits or bytes before doing the calculations and notethat no mathematical rounding should occur until the final answer,otherwise you will usually be off by some amount. You may ask the question now What does real Internet delay & loss looklike? There's a tool known as trace route, it's commonly used to becauseit can show us the route that a packet will take through the networkcore, here we call trace route with a web address that will resolve to aserver somewhere in Europe and we can see the different hops in therouting chain and also the measured delay between each hop.When the rate at which packets arrive at a link surpasses the rate atwhich the link can transmit them, packets will start to queue or wait ina buffer before being transmitted. As the queue fills up, packetsexperience queuing delay – the time they spend waiting in the buffer.This delay can lead to increased latency in the network. Now if thearrival rate continues to exceed the link's transmission rate, and thequeue reaches its maximum capacity (buffer overflow), packets may bedropped or lost. Packet loss can have significant implications fornetwork performance. It may lead to retransmissions, degradation ofquality for real-time applications, and overall reduced efficiency. Inaddition to delay, the probability of packet loss is helpful tocalculate. Finally, the performance metric throughput. Throughput is acrucial metric for evaluating network performance, and both instantaneousand average throughput provide valuable insights into the efficiency andcapacity of a network in handling data transfer between senders andreceivers. From this example, you can see that instantaneous throughputprovides insight into the speed of data transfer at a specific time,allowing for real-time monitoring and assessment. Whereas, if F bits takeT seconds to transfer from one endpoint to another, the averagethroughput is: F/T. It provides a straightforward way to assess theaverage data transfer rate over a given period, helping to quantify theefficiency and performance of a network or data transfer process.Let's break down the scenarios and discuss the implications for averageend-to-end throughput:The Scenario is: Rs?<Rc?Meaning that: The sender's transmission rate (Rs?) is less than thereceiver's reception rate (Rc?).Now the throughput of the communication is constrained by the sender'srate, and the network can only deliver data at the rate at which it isbeing sent. The slower of the two rates determines the overall averagethroughput.Consider the other Scenario: Rs?>Rc?Meaning: The sender's transmission rate (Rs?) is greater than thereceiver's reception rate (Rc?).Here we face a completely different problem. Although the sender cantransmit data at a higher rate, the receiver can only receive data at itsown rate. The average throughput is thus constrained by the slowerreception rate, and any excess data transmitted by the sender may bebuffered or dropped if the receiver cannot keep up.In summary, the average end-to-end throughput is constrained by theslower of the two rates (sender's transmission rate or receiver'sreception rate) in the communication, and the link with the lowestcapacity or highest delay in the end-to-end path may further limit theoverall throughput.Let’s consider this Internet scenario with 10 connections sharing abackbone bottleneck link with a capacity of R bits per second, theper-connection end-to-end throughput can be determined by considering theminimum of the individual transmission rates (Rc? for the receiver andRs? for the sender) and the available capacity of the bottleneck link(R/10 for each connection).The formula for per-connection end-to-end throughput can be expressed as:min(/10)min (Rc?,Rs?,R/10)In practice, the throughput for each connection is often constrained byeither the sender's transmission rate Rs?) or the receiver's receptionrate Rc?), depending on which is smaller. If Rc? or Rs? is smaller thanR/10 (the equal share for each connection on the bottleneck link), thenthe throughput will be limited by Rc? or Rs?.This scenario highlights the impact of the bottleneck link's capacity onthe overall throughput for each connection. The network's efficiency isoften limited by the slowest link or component in the communication path.Okay we are done here. Please take the practice quiz provided on theexploration page following this lecture. 